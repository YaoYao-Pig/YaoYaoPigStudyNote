# GAT代码复现

说是复现，其实主要是抄，然后读了一下人家实现的代码，再加上自己尝试着写了一下

尝试总结一下自己复现的感受：

首先是数据读取和预处理，我觉得数据的读取可以分为这几个阶段：

> 1. 数据文件的读入，这部分就是纯粹的把数据从文件当中读出来
> 2. 数据的提取。从读入的数据当中，把需要的数据分门别类的提取出来。比如节点的id，feature，label等等，这些都可以通过对数据的不同维度的提取来获得
> 3. 用`scipy.sparse`处理。这个过程中主要针对图数据，本来是稀疏矩阵，然后转换为稠密的格式来存储。比如：`features=sp.csr_matrix(idx_features_labels[:,1:-1],*dtype*=np.float32)`
> 4. 处理的目的是为了提取，构建图的结构。一般包括：节点的表示（id），节点的特征（feature），边的表示（edge），标签（label，在cora数据集当中是paper的分类）通过边的表示来构建adj。然后再把adj用稠密的格式存储
> 5. 归一化处理。对于这个例子来说，每个节点的feature是由多个关键词的存在与否来表示的（其实就是做了个one—hot），因此可以做一下归一化，让这些feature维度的数值总和为1。同时，对于adj来说，也需要归一化处理，但是方法不太一样，据说adj的处理方法可以让图更稳定。这里记录两种代码：
>
> ```python
> def nomalize_feature(features):
>     rowsum=np.array(features.sum(1))
>     r_inv=np.power(rowsum,-1).flatten()
>     
>     r_inv[np.isinf(r_inv)]=0.
> 
>     r_mat_inv = sp.diags(r_inv)
>     features=r_mat_inv.dot(features)
>     return features
> 
> def normalize_adj(mx):
>     """Row-normalize sparse matrix"""
>     rowsum = np.array(mx.sum(1))
>     r_inv_sqrt = np.power(rowsum, -0.5).flatten()
>     r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.
>     r_mat_inv_sqrt = sp.diags(r_inv_sqrt)
>     return mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt)
> ```
>
> 6. 张量（Tensor）化，让各种数据的存储结构变为Tensor的存储类型。方便调用cuda

然后是模型的构建。GAT本身比较简单。

> 模型用的是多头注意力，但是从代码上就是把一个自注意力用了n遍
>
> 这里更多的还是熟悉Pytorch使用：
>
> 1. 一个继承自nn.Module的类型主要就是两个部分：init和forward。init初始化的时候传入的参数记录。而forward就是真正计算的时候的内容。当模型被类似函数的调用的时候（比如GATLayer(x,adj)的时候，调用的就是forward部分）
> 2. 这里主要说forward，其实forward这里也就是一种数值计算。深度学习拆开来看没有什么无法理解的，就是用各种矩阵去做计算。本质上还是线性函数+非线性激活这两部分。线性函数也就无非是加减乘除concat。非线性激活也就是调调函数。我觉得最难得其实还是在如何操纵矩阵以及如何把矩阵的维度搞清楚。
> 3. 我觉得一个很关键的理解DL每一层的行为的方法是：一定要搞清楚每一层的输入输出到底是什么，而且是剥洋葱法：
>
> > 模型的最外层，最后的输出就是模型最终的目的，这部分是为了计算loss然后优化的。比如对于CORA数据集来说。模型最外层最终的输出就是七个类型的概率，因为是个分类任务
> >
> > 而对于内部的每一个注意力头，输入就是当前层的feature和adj而输出就也还是feature（adj在这里是不会变的，所以不需要输出），而目的是在内部的外面是需要再去处理这些不同头的输出的，可能还会再跟一个分类器，或者是一个输出的注意力。总之，搞清楚输出输出，才能搞清楚内部的矩阵维度变换

最后是训练：

> 这大概就是一个范式：
>
> 1. 参数的确定，包括训练epoch。优化器参数，输入输出的维度等等
> 2. 数据读取
> 3. 模型初始化（这里调用的就是init函数了）
> 4. 优化器的选择和初始化
> 5. cuda的初始化（包括了参数和设备）
> 6. 然后就可以开始train了。注意，需要被反向迭代优化的数据要被处理为Variable。然后就是这一套流程：
>
> ```python
>     model.train()
>     optimizer.zero_grad()
>     output=model(features,adj)
>     loss_train = F.nll_loss(output[idx_train], labels[idx_train])
>     acc_train = accuracy(output[idx_train], labels[idx_train])
>     loss_train.backward()
>     optimizer.step()
> ```

Calculate attention score We calculate these for each head $k$. *We have omitted $\cdot^k$ for simplicity*.

$$e_{ij} = a(\mathbf{W} \overrightarrow{h_i}, \mathbf{W} \overrightarrow{h_j}) =a(\overrightarrow{g_i}, \overrightarrow{g_j})$$    

$e_{ij}$ is the attention score (importance) from node $j$ to node $i$.

We calculate this for each head.

$a$ is the attention mechanism, that calculates the attention score.

The paper concatenates

$\overrightarrow{g_i}$, $\overrightarrow{g_j}$and does a linear transformation with a weight vector $\mathbf{a} \in \mathbb{R}^{2 F'}$

followed by a $\text{LeakyReLU}$.$$e_{ij} = \text{LeakyReLU} \Big(\mathbf{a}^\top \Big[\overrightarrow{g_i} \Vert \overrightarrow{g_j}\Big] \Big)$$

First we calculate

$\Big[\overrightarrow{g_i} \Vert \overrightarrow{g_j} \Big]$

for all pairs of $i, j$.`g_repeat` gets

$$\{\overrightarrow{g_1}, \overrightarrow{g_2}, \dots, \overrightarrow{g_N},\overrightarrow{g_1}, \overrightarrow{g_2}, \dots, \overrightarrow{g_N}, ...\}$$

where each node embedding is repeated `n_nodes` times.

Now we concatenate to get

$$\{\overrightarrow{g_1} \Vert \overrightarrow{g_1},\overrightarrow{g_1} \Vert \overrightarrow{g_2},\dots, \overrightarrow{g_1}  \Vert \overrightarrow{g_N},\overrightarrow{g_2} \Vert \overrightarrow{g_1},\overrightarrow{g_2} \Vert \overrightarrow{g_2},\dots, \overrightarrow{g_2}  \Vert \overrightarrow{g_N}, ...\}$$