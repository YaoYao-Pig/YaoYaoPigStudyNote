# Representation Learning on Graphs：Methods and Applications

| 项目 |                                                              |
| ---- | ------------------------------------------------------------ |
| 综述 | 是                                                           |
| 代码 |                                                              |
| 地址 | https://arxiv.org/abs/1709.05584v3                           |
| 亮点 |                                                              |
| 时间 | 2017                                                         |
| 级别 |                                                              |
| 参考 | [【论文笔记】图的表示学习：方法和应用_representation learning on graphs: methods and app-CSDN博客](https://blog.csdn.net/qq_42447107/article/details/116260521) |

## 问题：

把一个图的关联、节点特征等映射到一个高维空间的第一个点上

我的问题：

1. 有哪些常用方法？
2. 如何映射
3. 映射了之后如何使用？比如我后续的模型该怎么用？
4. 有什么提升，比如我直接使用模型的邻接矩阵为什么不如这种方法？

本文提出了一个框架：一个decoder，encoder架构

> 在这个框架中，我们围绕两个关键的映射函数来组织各种方法：编码器encoder，他将每个节点映射到一个低维向量或嵌入中；解码器decoder，它从学习到的嵌入中解码出关于图的结构信息 （图3）。编码器解码器背后的思想是：如果我们能从编码得到的低维嵌入中解码高维图信息（比如图中节点的全局位置信息和局部邻域信息），那么这些嵌入应该包含下游机器学习任务所需的所有信息。
>

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/7db34e0ddba69315f60cb61235d8d47f.png#pic_center)

【没看完，看不太懂，下次再看】

# Semantic and Structural View Fusion Modeling for Social Recommendation

| 项目 |                                                              |
| ---- | ------------------------------------------------------------ |
| 综述 | 是                                                           |
| 代码 | https://github.com/lcwy220/Social-Recommendation             |
| 地址 | https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9996130 |
| 亮点 | 【元路径】，隐式关系，一种新的类似dropout防止过拟合的方法。还加上了知识图谱 |
| 时间 | 2022                                                         |
| 级别 | A                                                            |
| 参考 | [[论文笔记\] 2022-TKDE-Semantic and Structural View Fusion Modeling for Social Recommendation-CSDN博客](https://blog.csdn.net/weixin_46448399/article/details/129328013) |







然而，尽管现有的社交推荐方法表现良好，但它们仍然存在两个限制。首先，众所周知，观察到的社会关系的稀疏性和不平衡分布阻碍了社交推荐的进一步改进。为了解决这个问题，一些工作转向挖掘用户和物品的隐含关系以丰富数据[14]，[15]。具体来说，可以在两个没有观察到社会关系但有相似偏好的用户之间建立用户隐含关系，而物品隐含关系指的是被同一用户偏好的物品。然而，构建社交关系的低成本和社交网络的开放性质可能会不可避免地带来巨大的噪音会降低显式和隐式关系的有效性。尽管存在一些其他方法专注于如何学习鲁棒的图结构，但很少有方法同时考虑了隐式关系增强和噪声减少。

此外，对于用户评分行为，不同级别的评分也表示了用户和物品在嵌入空间中的相对位置差异，即，高评分的物品应该比低评分的物品更接近用户。然而，这种归纳偏置并没有被明确保留并推广到其他类似的用户和物品，这阻碍了模型进一步的改进。

## 挑战

1. 社会关系的稀疏性和不平衡分布阻碍了社交推荐的进一步改进=>一些工作挖掘用户和物品的隐含关系，但是有噪声问题.(也就是说社交网络的加入，本身是为了推荐算法可以更好的找到推荐的对象，而且解决冷启动问题。但是社交网络自己本身可能就比较稀疏不够平衡)

2. 用户评分行为有问题（具体有啥我没看懂）（大概看懂了）就是说比如在评分任务上面，用户评分高的物品，距离用户应该更近，而用户评分低的物品应该更远。但是这个问题在之前的算法中没有解决（但是我觉得GraphRec里面的内个各种评分共享的偏置也很类似吧）（本文用了一个知识图谱的方法来解决）

   > 传统的相似度度量方法，如皮尔逊相关系数（PCC）[38]通常忽略了评分用户的数量。实际上，如果更多的用户以更接近的评分对两个项目进行评分，那么这两个项目可能更加相似，这在传统度量中无法体现。因此，为了揭示隐含的项目关系，我们构建了一个基于评分用户数量及其相应评分范围的相似度测量表。更正式地说，假设用户i分别给项目j和k的评分是rij和rik，我们首先根据用户i定义项目j和k之间的相似度sjk。

> 社交推荐算法的优化逻辑：加强相似的物品之间的联系，加强相似人之间的联系，强化有关系的人和物品之间的联系

## 论文

1. 对于用户的社交关系：使用了隐含的用户关系来加强用户的社交推荐（两个没有直接联系的用户，有共同的追随者，这些追随者是由于共享的偏好而等同）
2. 对于物品关系：隐含的物品关系指的是被相同的用户打出了类似的分数的物品，这些物品更加接近

3. 提出了一个HIN网络，多了几张图，user-item，user-explicit users，user implicit users
4. 因为图太多了， 所以可能导致嘈杂关系的过拟合（我的理解就是因为关系是在太多，所以每个人每个物品之间都有或多或少的关系，导致推荐的时候大家权重都差不多，也很难推荐，所以也是一个启示），论文用了一个随机保留丢弃算法
5. 这个思路的整理很好，
   1. 对于社交关系参与的评分预测来说，最开始是一些线性的模型：就是我的评分应该是和我的朋友的评分的线性组合（比如平均啊，加权平均啊），我的评分被强制和朋友类似。
   2. 下一步就是随着DL的兴起了，模型带来了非线性，但是过去的方法呢，都是在一些给出的图当中学习，还有大量的隐式关系没有利用到（这个好好）



## 泛化性

本文的泛化性是一个很重要的部分，首先就是如果meta-path定义的隐式关系图全都看，那么对于一个热门物品来说，他会非常的庞大， 并且容易过拟合，所以首先就是做了dropout

其次，如果只关注dropout之后生成的局部视图，那么还是更受欢迎的用户和物品就会“绑架”所有人，所以为了纠正这部分偏差，也就需要用最开始那种one-hop定义的一阶关系（就是最直接的邻居和邻居物品）来从全局的试图来描述，维护评分稳定。（我暂时的理解就是，隐式关系和显式的一阶关系要分开处理，权重也不能一样）

## 元路径聚合

聚合是一个递推的过程，比如u-i-u这个聚合，就要先聚合第一个u-i，聚合用户到item的关系，（过一个GAT），然后再用聚合后的数据再聚合i-u的关系，再过一个GAT

![image-20240921134359228](../../../../AppData/Roaming/Typora/typora-user-images/image-20240921134359228.png)

深度模块：尽量深的找到显示和隐式关系（但是使用了dropout，丢掉了一些节点，所以广度不够），但是缺点是容易被一些热门项目绑架

广度模块：没有用droput，但是为了防止过拟合，就只找第一跳的附近节点（包括隐式和显式），广度很广但是深度不足，聚合方法也特别简单，就是单纯的加和，最后r的预测也不是通过MLP，而是直接做了一个线性的dot，这样最大限度的保留模型的泛化性。（要结合下面的式子来看这个图，对于一个节点$u_i$，做wideLinear Model的时候，最后的目标是获得一个embed ing的向量。首先需要的是$u_i$自己的embedding，也就是$p_i^{'}$,然后要聚合周围的显示或者隐式的信息。按照DL的做法，这个时候要融合$p_i^{'}$和周围的u或者v的embedding，做个MLP啊，GAT啊，但是为了保证泛化性，这里的聚合就直接求和就可以了。下面式子的每一个求和项，都是直接加上自己邻居的那个embedding值。然后最后预测的时候就是直接×一个周围item结点和隐式item节点的点乘就可以了。也不需要MLP

![image-20240921142316099](../../../../AppData/Roaming/Typora/typora-user-images/image-20240921142316099.png)
