# GAT：Graph Attention Networks【精读】

| 项目 |                                                              |
| ---- | ------------------------------------------------------------ |
| 综述 | 否                                                           |
| 代码 |                                                              |
| 地址 | [[1710.10903\] Graph Attention Networks (arxiv.org)](https://arxiv.org/abs/1710.10903) |
| 亮点 |                                                              |
| 时间 | 2017                                                         |
| 参考 |                                                              |

首先学到了一个分类问题

从“处理域”上，GNN分为两种处理方法：

1. 一种是谱图神经网络，比如GCN，在频域上进行处理；
2. 另一种是在时域（词不太准确）上进行直接处理，GAT就是。

然后就是研究对象的分类

1. 一种的研究对象是模型训练在什么图上，最后应用也不变。一般是对图进行分类对图上的节点的一些参数做预测。叫做transductive直推式学习任务。（通过已经标记的节点预测未标记节点，但是结构是已知的）
2. 第二种研究对象就是训练是在一个图上面，但是最后的应用要迁移到新的图上面去做，叫做inductive归纳式



对于注意力机制，我有一套新的感受：在图上的运用我的理解是来自对于卷积的改进。

首先卷积和Att都是一种对于【聚合】操作的改进（或者方法）。卷积操作的更多的关注的是位置，对于任意一个节点，它周围固定位置上的节点的权重在学习后是一致的。（我觉得也是有道理的，因为距离节点近说明彼此之间关系密切）。

而ATT本身，则是通过对于周围节点的【特征】的数值来分配权重的。特征向量有很多维度，而注意力会学习出一个矩阵，来表明模型在每个维度上分配的权重。通过这个权重和特征值的关系最后求一个加和，就可以获得一个数值（也就是注意力），用来描述某个节点对于周围每个节点应该分配多少注意力在上面（因为求了一个softmax做了归一化，所以最后出来的是可以适配多种不同结构的网络的）。

这样解释下来：

Key矩阵代表的是模型对于不同特征的关注程度

value代表的是关注后的结果

query：是对一些事物某一些任务的关注（这个不一定准确，还要再想）

## 论文：

### 背景：

1. 基于以前谱的方法的问题：首先是以前的方法依赖拉普拉斯矩阵，依赖图的结构，所以模型不能应用在不同其他结构的图上面
2. 以前基于非谱的方法：有人定义了一个大小实用不同邻居但是又保证权重共享的卷积。。。



### 方法

用的是masked attention，所以每个节点只需要关注自身周围的邻居节点就可以了，不需要关注全图。

同时用的多头注意力。（文中的理由是为了“稳定”自注意力）



模型的“感受野”受到了模型深度的影响（这个和GCN很像，就是每一层只多关注一阶邻居，那么有几层最后就能聚合多少层之外的节点信息）
