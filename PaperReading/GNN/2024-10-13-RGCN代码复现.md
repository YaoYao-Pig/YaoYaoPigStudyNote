# RGCN代码复现

这次复现过程中虽然也是以抄为主，但是也加入了更多自己的思考

我有一些新的感受在这里记录：

## Embedding

在代码中，需要对节点和链接关系进行embedding，这里先讨论embedding做了什么

代码中使用的是`nn.embedding`函数，这个函数主要是用于把一个一维的序列，映射到一个高维度的特征空间。这里的一维序列主要就是节点的序号`[0,1,2,3...]`，一开始我有这样的疑惑：由节点序号映射的embedding，代表的不就是节点本身吗，和特征等等语义信息有什么关系。但是，embedding矩阵是用来做计算的，而矩阵本身在模型学习过程中也是会不断改变的。

>  embedding过程应该被认为是这样的一个过程：通过一个embedding矩阵运算，把一个节点，通过唯一表示符（也许是id，是序号，是name。。）映射到一个高维度的特征空间当中。
>
> embedding矩阵学习到的就是去捕捉如何映射一个唯一节点到高纬度空间当中的过程，这个映射过程就可以在学习中获取



Node2Vec是一种embedding的方法，参考了Word2Vec，把图的连接关系表示成类似一句话的那样的序列。然后学习映射的关系。

## Normalize

归一化的操作往往是在图当中，通过平衡不同节点的链接性的一种操作。有的节点重要，链接多，有的节点链接少，如果不处理的话容易导致某些权重特别大

## 负样本

在图的训练的时候，经常会在数据预处理节点生成负样本，也就是“真实图当中不存在的链接关系”，这时候会给一个label来标注是否存在，负样本可以让模型感知到不真实形式到底是什么样子的

## 数据预处理

往往都会把一些字段映射为纯数字的id的形式



## 卷积与Attention

Attention：

本质上，就是有一个矩阵W，这个W的维度是[feature,1]，也就是说，把特征值转换为一个参数，这个参数就是attention的权重（当然别忘了softmax）

卷积：

原始的聚合是很多个feature加在一起然后平均啊或者根本就不平均了。卷积的方法就是再搞一个矩阵W，W的维度是【feature_in,feature_out】，目的是进一步通过学习的方法聚合特征，最后再加起来或者平均之类的。

